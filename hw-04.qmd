---
title: "Regression in R"
description: "Basic Concepts Regression Techniques"
author: "Bharath Velamala"
format: 
    html:
      theme: yeti
editor: visual
toc: true
code-overflow: wrap
code-annotations: hover
execute: 
  warning: false
---

## Install packages

Installing the packages used.

```{r r_packages, message = FALSE, output=FALSE}
#| code-fold: true
#| code-summary: "Packages and Theme Settings"

# Required packages
if (!require(pacman))
  install.packages("pacman")

pacman::p_load(tidymodels,
               tidyverse,
               ranger,
               dlookr,
               randomForest,
               formattable,
               glmnet,
               gridExtra)

# Global ggplot theme
# setting theme for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 15, base_family = "sans"))

# setting width of code output
options(width = 65)

# setting figure parameters for knitr
knitr::opts_chunk$set(
  fig.width = 8,        # 8" width
  fig.asp = 0.65,       # the golden ratio
  fig.retina = 1,       # dpi multiplier for displaying HTML output on retina
  fig.align = "center", # center align figures
  dpi = 150,            # higher dpi, sharper image
  message = FALSE
)
```

## Tidy Tuesday Dataset

I have selected the data [Big Stock Prices](https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-02-07) from TidyTuesday which was sourced from Yahoo Finance via [Kaggle](https://www.kaggle.com/datasets/evangower/big-tech-stock-prices). This dataset consists of the daily stock prices and volume of 14 different tech companies, including Apple (AAPL), Amazon (AMZN), Alphabet (GOOGL), and Meta Platforms (META) and more. I will be looking for answers or some patterns by considering the below question.

**How do daily opening prices, trading volumes, and historical trends influence the adjusted closing prices of stocks?**

```{r nombats_dataset, message=FALSE}
big_stocks <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-07/big_tech_stock_prices.csv')

# converting the data to tibble
big_stocks <- as_tibble(big_stocks)

big_stocks |>
  diagnose() |>
  formattable()
```

By the above diagnosis, we can interpret that there are no missing values in the data. First we will generate a linear fit to the data by assuming `X` as `open` and `Y` as `adj_close`.

```{r input_output, message=FALSE}
# Setting parameters
seed <- 1               # seed for random number generation 
numInstances <- nrow(big_stocks)  # number of data instances

# Setting seed
set.seed(seed)

X <- big_stocks$open
# adding noise to the data
Y_true <- big_stocks$adj_close
Y <- Y_true + matrix(rnorm(numInstances), ncol =1)
```

Added noise to the `variable` above using a normal distribution.

```{r linear_fit, message=FALSE}
# Plotting Linear Fit 
ggplot(big_stocks, aes(X, Y)) +
  geom_point(color = "black") +
  geom_smooth(method = "lm",
              color = "red",
              linewidth = 1) +
  labs(
    title = "Stock Price relationship between open and adjusted closing price",
    x = "Open Price",
    y = "Adjusted Closing Price"
  )
```

***Interpretation:***It can be interpreted that in most cases when ever there is raise in `open` price, `adj_close` is also experiencing a raise in it's price. Which can be determined as a linear relationship between `open` and `adj_close` based on the above plot.

## Multiple Linear Regression

Given the input dataset, the following steps are performed:

1.  Split Input Data into Training and Test Sets
2.  Fit Regression Model to Training Set
3.  Apply Model to the Test Set
4.  Evaluate Model Performance on Test Set
5.  Post-processing: Visualizing the model fit

### Step 1: Split Input Data into Training and Test Sets

We have a total of `r numInstances`, so we will be using around `25000` records as training data and the rest as resting data.

```{r split_input_data, message=FALSE}
set.seed(123) # For reproducibility

# defining train and test data
numTrain <- 10000
numTest <- numInstances - numTrain

stocks_data <- tibble(X = X, Y = Y)

split_stocks <- initial_split(stocks_data, prop = numTrain / numInstances)

# separating train and test data
train_stocks <- training(split_stocks)
test_stocks <- testing(split_stocks)
```

We will be creating training set `X_train` and `Y_train` and testing set `X_test` and `Y_test`.

```{r X_Y_TrainTest, message=FALSE}
# separating X_train, X_test, Y_train, Y_test
X_train <- train_stocks$X
Y_train <- train_stocks$Y

X_test <- test_stocks$X
Y_test <- test_stocks$Y
```

### Step 2: Fit Regression Model to Training Set

Using `linear_reg()` to create a linear regression model by setting `lm`.

```{r linear_regression, message=FALSE}
# creating a linear regression model specification
lin_reg_spec <- linear_reg() |>
  set_engine("lm")

# fitting the model to the training data
lin_reg_fit <- lin_reg_spec |>
  fit(Y ~ X, data = train_stocks)
```

Fitting the data where our data is `train_stocks`, Y is `adj_close` and X is `open`.

### Step 3: Apply Model to the Test Set

Predicting the test data outcome using the linear regression fit in above section.

```{r model_pred, message=FALSE}
# applying model to the test set
Y_pred_test <- predict(lin_reg_fit, new_data = test_stocks) |>
  pull(.pred)
```

### Step 4: Evaluate Model Performance on Test Set

Plotting the data to compare true and predicted values of test dataset.

```{r model_pref_plot, message=FALSE}
# Plotting true vs predicted values using ggplot
ggplot() +
  geom_point(aes(x = as.vector(Y_test), y = Y_pred_test), color = 'black') +
  labs(
    title = "Comparing true and predicted values for test set",
    x = "True values for Y (adjusted closing price)",
    y = "Predicted values for Y (adjusted closing price)"
  )
```

***Interpretation:*** From the above scatter plot of `Y_test` which is true value of adjusted closing price and `Y_pred_test` which is the predicted value of adjusted closing price. We can observe that the model performed well as it is displaying a linear relationship between those two, which can be interpreted as the predicted value is closer to the true value.

```{r linear_model_eval, message=FALSE}
# preparing data for yardstick evaluation
eval_data <- tibble(truth = as.vector(Y_test),
                    estimate = Y_pred_test)

# Model evaluation
rmse_value <-
  rmse(data = eval_data,
       truth = truth,
       estimate = estimate)
r2_value <- rsq(eval_data, truth = truth, estimate = estimate)

cat("Root mean squared error =",
    sprintf("%.4f", rmse_value$.estimate),
    "\n")

cat('R-squared =', sprintf("%.4f", r2_value$.estimate), "\n")
```

***Interpretation:*** We can observe from the output that the `root mean squared error (RMSE)` of `10.6773`, from which we can understand that, on average the model's predictions show a deviation of 10.6773 from true values.\
And the `R-squared` value of `0.9885` signifies that the model captures approximately `98.85%` of the variability in the response variable, reflecting good predictive accuracy.

### Step 5: Post-processing: Visualizing the model fit

```{r post_processing, message=FALSE}
# displaying model parameters
coef_values <- coef(lin_reg_fit$fit)  # extracting coefficients
slope <- coef_values["X"]
intercept <- coef_values["(Intercept)"]

cat("Slope =", slope, "\n")

cat("Intercept =", intercept, "\n")
```

***Interpretation:*** The `slope` of `0.9888` signifies the rate of change in the variable which we are predicting (`adj_close)` per increase in the variable which we are using for the prediction as a feature(`open`). The `intercept` of `-3.0146` is the estimated value of the `adj_close` when the `open` is zero. Together, these coefficients define the linear relationship of the model.

```{r plotting_predicted, message=FALSE}
# plotting outputs
ggplot() +
  geom_point(aes(x = as.vector(X_test), y = as.vector(Y_test)), color = 'black') +
  geom_line(aes(x = as.vector(X_test), y = Y_pred_test),
            color = 'blue',
            linewidth = 1) +
  labs(
    title = sprintf('Predicted Function: y = %.2fX + %.2f', slope, intercept),
    x = "X (opening price)",
    y = "Y (adjusted closing price)"
  )
```

***Interpretation:*** In this plot, data points are actual data points and blue line are predicted values. The line is used to represent the relationship between the opening and adjusted closing prices. We can understand that the relationship as a strong positive correlation between the opening(`open`) and adjusted closing(`adj_close`) prices.

## Effect of Correlated Attributes

In this section, I will be introducing other attributes which might effect the `adj_close` price and use attributes that are strongly correlated with the previous variable X created. The association between X and Y remains consistent. Subsequently, we proceed to model Y in relation to the predictor variables and assess the training and test set errors for comparison.

Getting correlation matrix to understand the correlation among the features.

```{r get_correlation, message=FALSE}
big_stocks |>
  plot_correlate()
```

From the above above correlation matrix we can determine that `high`, `low`, `close`, `open`, `adj_close` are highly positively correlated. And `volume` seems to be not strongly correlated.

```{r correla_attr, message = FALSE}
set.seed(1)
X2 <- big_stocks$high
X3 <- big_stocks$low
X4 <- big_stocks$close
X5 <- big_stocks$volume

plot1 <- ggplot() +
  geom_point(aes(X, X2), color = 'black') +
  xlab('Opening Price (X)') + ylab('Highest Price (X2)') +
  ggtitle(sprintf("Correlation between open and high = %.4f", cor(X[-c((numInstances -
                                                                     numTest + 1):numInstances)], X2[-c((numInstances - numTest + 1):numInstances)])))

plot2 <- ggplot() +
  geom_point(aes(X2, X3), color = 'black') +
  xlab('Highest Price (X2)') + ylab('Lowest Price (X3)') +
  ggtitle(sprintf("Correlation between high and low = %.4f", cor(X2[-c((numInstances -
                                                                       numTest + 1):numInstances)], X3[-c((numInstances - numTest + 1):numInstances)])))

plot3 <- ggplot() +
  geom_point(aes(X3, X4), color = 'black') +
  xlab('Lowest Price (X3)') + ylab('CLosest Price (X4)') +
  ggtitle(sprintf("Correlation between low and close = %.4f", cor(X3[-c((numInstances -
                                                                       numTest + 1):numInstances)], X4[-c((numInstances - numTest + 1):numInstances)])))

plot4 <- ggplot() +
  geom_point(aes(X4, X5), color='black') +
  xlab('Closing price (X4)') + ylab('Volume (X5)') +
  ggtitle(sprintf("Correlation between close and volume = %.4f", cor(X4[-c((numInstances-numTest+1):numInstances)], X5[-c((numInstances-numTest+1):numInstances)])))

# combining plots into a 2x2 grid
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

We can observe the same as the correlation matrix which is:

We will be involving `open`, `high`, `close` and `low` as these are strongly correlated, even though `volume` is weakly correlated will be using it for predictions.

```{r train_test_data, message = FALSE}
train_indices <- 1:(numInstances - numTest)
test_indices <- (numInstances - numTest + 1):numInstances

X_train2 <- cbind(X[train_indices], X2[train_indices])
X_test2 <- cbind(X[test_indices], X2[test_indices])

X_train3 <-
  cbind(X[train_indices], X2[train_indices], X3[train_indices])
X_test3 <-
  cbind(X[test_indices], X2[test_indices], X3[test_indices])

X_train4 <-
  cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices])
X_test4 <-
  cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices])

X_train5 <-
  cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices], X5[train_indices])
X_test5 <-
  cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices], X5[test_indices])
```

Below, we train 3 new regression models based on the 4 versions of training and test data created in the previous step.

```{r test_reggression, message = FALSE}
train_data2 <-
  tibble(X1 = X_train2[, 1], X2 = X_train2[, 2], y = Y_train)

train_data3 <-
  tibble(X1 = X_train3[, 1],
         X2 = X_train3[, 2],
         X3 = X_train3[, 3],
         y = Y_train)

train_data4 <-
  tibble(
    X1 = X_train4[, 1],
    X2 = X_train4[, 2],
    X3 = X_train4[, 3],
    X4 = X_train4[, 4],
    y = Y_train
  )

train_data5 <-
  tibble(
    X1 = X_train5[, 1],
    X2 = X_train5[, 2],
    X3 = X_train5[, 3],
    X4 = X_train5[, 4],
    X5 = X_train5[, 5],
    y = Y_train
  )

regr2_spec <- linear_reg() |> set_engine("lm")
regr2_fit <- regr2_spec |> fit(y ~ X1 + X2, data = train_data2)

regr3_spec <- linear_reg() |> set_engine("lm")
regr3_fit <-
  regr3_spec |> fit(y ~ X1 + X2 + X3, data = train_data3)

regr4_spec <- linear_reg() |> set_engine("lm")
regr4_fit <-
  regr4_spec |> fit(y ~ X1 + X2 + X3 + X4, data = train_data4)

regr5_spec <- linear_reg() |> set_engine("lm")
regr5_fit <-
  regr5_spec |> fit(y ~ X1 + X2 + X3 + X4 + X5, data = train_data5)
```

All 3 versions of the regression models are then applied to the training and test sets.

```{r new_data_pred, message = FALSE}
new_train_data2 <- setNames(as.data.frame(X_train2), c("X1", "X2"))
new_test_data2 <- setNames(as.data.frame(X_test2), c("X1", "X2"))

new_train_data3 <-
  setNames(as.data.frame(X_train3), c("X1", "X2", "X3"))
new_test_data3 <-
  setNames(as.data.frame(X_test3), c("X1", "X2", "X3"))

new_train_data4 <-
  setNames(as.data.frame(X_train4), c("X1", "X2", "X3", "X4"))
new_test_data4 <-
  setNames(as.data.frame(X_test4), c("X1", "X2", "X3", "X4"))

new_train_data5 <-
  setNames(as.data.frame(X_train5), c("X1", "X2", "X3", "X4", "X5"))
new_test_data5 <-
  setNames(as.data.frame(X_test5), c("X1", "X2", "X3", "X4", "X5"))

y_pred_train2 <- predict(regr2_fit, new_data = new_train_data2)
y_pred_test2 <- predict(regr2_fit, new_data = new_test_data2)

y_pred_train3 <- predict(regr3_fit, new_data = new_train_data3)
y_pred_test3 <- predict(regr3_fit, new_data = new_test_data3)

y_pred_train4 <- predict(regr4_fit, new_data = new_train_data4)
y_pred_test4 <- predict(regr4_fit, new_data = new_test_data4)

y_pred_train5 <- predict(regr5_fit, new_data = new_train_data5)
y_pred_test5 <- predict(regr5_fit, new_data = new_test_data5)
```

For post-processing, we compute both the training and test errors of the models. We can also show the resulting model and the sum of the absolute weights of the regression coefficients, i.e., $\sum_{j=0}^d |w_j|$, where $d$ is the number of predictor attributes.

```{r coef_functions, message=FALSE}
get_coef <- function(model) {
  coef <- coefficients(model$fit)
  coef
}

calculate_rmse <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted) ^ 2))
  rmse
}
```

```{r results_regression, message=FALSE}
results <- tibble(
  Model = c(
    sprintf("%.2f X + %.2f", get_coef(regr2_fit)['X1'], get_coef(regr2_fit)['(Intercept)']),
    sprintf(
      "%.2f X + %.2f X2 + %.2f",
      get_coef(regr3_fit)['X1'],
      get_coef(regr3_fit)['X2'],
      get_coef(regr3_fit)['(Intercept)']
    ),
    sprintf(
      "%.2f X + %.2f X2 + %.2f X3 + %.2f",
      get_coef(regr4_fit)['X1'],
      get_coef(regr4_fit)['X2'],
      get_coef(regr4_fit)['X3'],
      get_coef(regr4_fit)['(Intercept)']
    ),
    sprintf(
      "%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f",
      get_coef(regr5_fit)['X1'],
      get_coef(regr5_fit)['X2'],
      get_coef(regr5_fit)['X3'],
      get_coef(regr5_fit)['X4'],
      get_coef(regr5_fit)['(Intercept)']
    )
  ),
  
  Train_error = c(
    calculate_rmse(Y_train, y_pred_train2$.pred),
    calculate_rmse(Y_train, y_pred_train3$.pred),
    calculate_rmse(Y_train, y_pred_train4$.pred),
    calculate_rmse(Y_train, y_pred_train5$.pred)
  ),
  
  Test_error = c(
    calculate_rmse(Y_test, y_pred_test2$.pred),
    calculate_rmse(Y_test, y_pred_test3$.pred),
    calculate_rmse(Y_test, y_pred_test4$.pred),
    calculate_rmse(Y_test, y_pred_test5$.pred)
  ),
  
  Sum_of_Absolute_Weights = c(sum(abs(
    get_coef(regr2_fit)
  )),
  sum(abs(
    get_coef(regr3_fit)
  )),
  sum(abs(
    get_coef(regr4_fit)
  )),
  sum(abs(
    get_coef(regr5_fit)
  )))
)
```

```{r plot_results_regr, message=FALSE}
ggplot(results, aes(x = Sum_of_Absolute_Weights)) +
  geom_line(aes(y = Train_error, color = "Train error"), linetype = "solid") +
  geom_line(aes(y = Test_error, color = "Test error"), linetype = "dashed") +
  labs(x = "Sum of Absolute Weights", y = "Error rate") +
  theme_minimal()

results
```

## Ridge Regression

```{r ridge_regression, message=FALSE}
train_data <- tibble(y = Y_train, X_train5)
test_data <- tibble(y = Y_test, X_test5)

ridge_spec <- linear_reg(penalty = 0.4, mixture = 1) |>
  set_engine("glmnet")

ridge_fit <- ridge_spec |>
  fit(y ~ ., data = train_data)

y_pred_train_ridge <-
  predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = test_data)$.pred

calculate_rmse <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted) ^ 2))
  rmse
}

ridge_coef <- coefficients(ridge_fit$fit)

model6 <-
  sprintf(
    "%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f",
    ridge_coef[2],
    ridge_coef[3],
    ridge_coef[4],
    ridge_coef[5],
    ridge_coef[6],
    ridge_coef[1]
  )

values6 <- tibble(
  Model = model6,
  Train_error = calculate_rmse(Y_train, y_pred_train_ridge),
  Test_error = calculate_rmse(Y_test, y_pred_test_ridge),
  Sum_of_Absolute_Weights = sum(abs(ridge_coef))
)

final_results <- bind_rows(results, values6)

final_results
```

## Lasso Regression

```{r lasso_reggression_training, message=FALSE}
lasso_spec <- linear_reg(penalty = 0.02, mixture = 1) |>
  set_engine("glmnet")

train_data <-
  tibble(
    y = Y_train,
    X1 = X_train5[, 1],
    X2 = X_train5[, 2],
    X3 = X_train5[, 3],
    X4 = X_train5[, 4],
    X5 = X_train5[, 5]
  )

lasso_fit <- lasso_spec |>
  fit(y ~ ., data = train_data)

lasso_coefs <- lasso_fit$fit$beta[, 1]
lasso_coefs
```

```{r lasso_reg_pred, message=FALSE}
y_pred_train_lasso <-
  predict(lasso_fit, new_data = train_data)$.pred
y_pred_test_lasso <-
  predict(
    lasso_fit,
    new_data = tibble(
      X1 = X_test5[, 1],
      X2 = X_test5[, 2],
      X3 = X_test5[, 3],
      X4 = X_test5[, 4],
      X5 = X_test5[, 5]
    )
  )$.pred

model7 <-
  sprintf(
    "%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f",
    lasso_coefs[2],
    lasso_coefs[3],
    lasso_coefs[4],
    lasso_coefs[5],
    lasso_coefs[6],
    lasso_fit$fit$a0[1]
  )

values7 <- c(model7,
             sqrt(mean((
               Y_train - y_pred_train_lasso
             ) ^ 2)),
             sqrt(mean((
               Y_test - y_pred_test_lasso
             ) ^ 2)),
             sum(abs(lasso_coefs[-1])) + abs(lasso_fit$fit$a0[1]))
```

```{r lasso_results, message=FALSE}
lasso_results <- tibble(
  Model = "Lasso",
  `Train error` = values7[2],
  `Test error` = values7[3],
  `Sum of Absolute Weights` = values7[4]
)

lasso_results
```

## Hyper-parameter Selection via Cross-Validation

### Fitting a ridge regression model by selecting the best hyper-parameter value

```{r hyper_params, message=FALSE}
Y_train <- as.vector(Y_train)

train_data <- tibble(y = Y_train, X1 = X_train5[,1], X2 = X_train5[,2], 
                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])

recipe_obj <- recipe(y ~ ., data = train_data) |>
  step_normalize(all_predictors()) |>
  prep()
```

```{r ridge_workflow, message=FALSE}
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) |>
  set_engine("glmnet")

ridge_wf <- workflow() |>
  add_model(ridge_spec) |>
  add_recipe(recipe_obj)

# alphas for ridge regression
alphas <- tibble(penalty = c(0.2, 0.4, 0.6, 0.8, 1.0))

# tuning the results for ridge regression
tune_results <- 
  ridge_wf |>
  tune_grid(
  resamples = bootstraps(train_data, times = 5),
  grid = alphas
)
```

```{r best_params_fit, message=FALSE}
best_params <- tune_results |> select_best("rmse")

ridge_fit <- ridge_spec |>
  finalize_model(best_params) |>
  fit(y ~ ., data = train_data)

ridge_coefs <- ridge_fit$fit$beta[,1]

y_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], 
                                                          X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred
```

```{r creating_model_str, message=FALSE}
model6 <- sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f", 
                  ridge_coefs[2], ridge_coefs[3], ridge_coefs[4], 
                  ridge_coefs[5], ridge_coefs[6], ridge_fit$fit$a0[1])

values6 <- c(model6, 
             sqrt(mean((Y_train - y_pred_train_ridge)^2)),
             sqrt(mean((Y_test - y_pred_test_ridge)^2)),
             sum(abs(ridge_coefs[-1])) + abs(ridge_fit$fit$a0[1]))
```

```{r ridge_results, message=FALSE}
ridge_results <- tibble(Model = "RidgeCV",
                        `Train error` = values6[2], 
                        `Test error` = values6[3], 
                        `Sum of Absolute Weights` = values6[4])

cat("Selected alpha =", best_params$penalty, "\n")

all_results <- bind_rows(results, ridge_results)
all_results
```

### Fitting a lasso regression model by selecting the best hyper-parameter value

```{r setting_data, message=FALSE}
set.seed(1234)

Y_train <- as.vector(Y_train)

train_data <- tibble(y = Y_train, X1 = X_train5[,1], X2 = X_train5[,2], 
                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])

recipe_obj_lasso <- recipe(y ~ ., data = train_data) |>
  step_normalize(all_predictors()) |>
  prep()
```

```{r lasso_specs, message=FALSE}
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")

lasso_wf <- workflow() |>
  add_recipe(recipe_obj_lasso)

lasso_fit <- lasso_wf |>
  add_model(lasso_spec) |>
  fit(data = train_data)

lambda_grid <- grid_regular(penalty(), levels = 50)

tune_results_lasso <- 
  tune_grid(lasso_wf |> add_model(lasso_spec),
  resamples = bootstraps(train_data, times = 5),
  grid = lambda_grid
)

best_params_lasso <- tune_results_lasso |> select_best("rmse")

lasso_fit <- lasso_spec |>
  finalize_model(best_params_lasso) |>
  fit(y ~ ., data = train_data)

lasso_coefs <- lasso_fit$fit$beta[,1]
```

```{r predict_lasso, message=FALSE}
y_pred_train_lasso <- predict(lasso_fit, new_data = train_data)$.pred
y_pred_test_lasso <- predict(lasso_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], 
                                                          X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred

model7 <- sprintf("%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f", 
                  lasso_coefs[2], lasso_coefs[3], lasso_coefs[4], 
                  lasso_coefs[5], lasso_coefs[6], lasso_fit$fit$a0[1])

values7 <- c(model7, 
             sqrt(mean((Y_train - y_pred_train_lasso)^2)),
             sqrt(mean((Y_test - y_pred_test_lasso)^2)),
             sum(abs(lasso_coefs[-1])) + abs(lasso_fit$fit$a0[1]))
```

```{r results_lasso_hp, message=FALSE}
lasso_results <- tibble(Model = "LassoCV",
                        `Train error` = values7[2], 
                        `Test error` = values7[3], 
                        `Sum of Absolute Weights` = values7[4])

cat("Selected alpha for Lasso =", best_params_lasso$penalty, "\n")

lasso_results
```
